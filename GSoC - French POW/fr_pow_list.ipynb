{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from textblob2 import load, save\n",
    "from itertools import chain, combinations\n",
    "import random\n",
    "import requests as r\n",
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import langid\n",
    "import csv\n",
    "import re\n",
    "import py8chan as pc\n",
    "from pathlib import Path\n",
    "from pattern.text import parse, language, find_tokens\n",
    "from pattern.text.fr import tokenize as tokenize_fr\n",
    "from pattern.text.fr import tag as tag_fr\n",
    "from collections import Counter, defaultdict\n",
    "import grasp.grasp as grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0\"}\n",
    "\n",
    "ANDROID = \"android_offensive_words.txt\"\n",
    "CRUCI_GR = \"cruciverbistes_grossièreté.txt\"\n",
    "CRUCI_NATIO = \"cruciverbistes_insultes_natio.txt\"\n",
    "CRUCI_GEN = \"crucivervistes_inusltes_general.txt\"\n",
    "WIKI = \"wiki_insultes.txt\"\n",
    "INSULT_FILES = [ANDROID,CRUCI_GR,CRUCI_NATIO,CRUCI_GEN,WIKI]\n",
    "\n",
    "SMS88 = \"88milSMS.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>8chan Scraping </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER8 = r\"C:\\Users\\Pierre\\Documents\\@ADMIN\\#KUL - IA\\_Stage\\TexTGaiN\\GSoC\\French POW List\\8chan_fr\"\n",
    "\n",
    "CHAN8 = \"https://8ch.net/{}/index.html\"\n",
    "# DEMPART_URL_RE = re.compile(r\"https://8ch.net/dempart/res/\\d{6}\")\n",
    "# DEMPART_URL = \"https://8ch.net/dempart/2.html\"\n",
    "SEARCH8 = \"https://8ch.net/search.php?search={}&board={}\"\n",
    "\n",
    "CATALOG8 = \"https://8ch.net/{}/catalog.html\"\n",
    "ARCHIVE8 = \"https://8ch.net/{}/archive/index.html\"\n",
    "THREAD_RE = re.compile(r\"\\\"id\\\":\\\"<a href=\\\\\\\"/\\w+/res/(\\d+)\\.html\\\\\\\"\")\n",
    "\n",
    "OP = re.compile(r\"op_\\d{6}\")\n",
    "REPLY = re.compile(r\"reply_\\d{6}\")\n",
    "GREEN_QUOTE = re.compile(r\">.*$\")\n",
    "\n",
    "CSV_HEADER = ['id',\"type\", \"text\"]\n",
    "\n",
    "\n",
    "with open(\"8ch_all_boards.txt\", \"r\") as f:\n",
    "    BOARDS = f.read().split(\"\\n\")\n",
    "with open(\"fr_boards_with_langid.txt\", \"r\") as f:\n",
    "    FR_BOARDS = f.read().split(\"\\n\")\n",
    "    \n",
    "CH8_PREPROCESSED = r\"C:\\Users\\Pierre\\Documents\\@ADMIN\\#KUL - IA\\_Stage\\TexTGaiN\\GSoC\\French POW List\\8chan_fr\\all_boards_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_results(bs_response):\n",
    "    \"\"\"used to check whether a search with SEARCH8 yields results or not\"\"\"\n",
    "    res = bs_response.find(\"legend\")\n",
    "    if res:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JeuxVideos.com Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JVBASE = \"http://www.jeuxvideo.com/\"\n",
    "JVSEARCH = \"http://www.jeuxvideo.com/recherche/forums/0-51-0-1-0-{}-0-blabla-18-25-ans.htm?search_in_forum={}&type_search_in_forum=texte_message\"\n",
    "JVFOLDER = r\"C:\\Users\\Pierre\\Documents\\@ADMIN\\#KUL - IA\\_Stage\\TexTGaiN\\GSoC\\French POW List\\JV\"\n",
    "JVPOL = \"http://www.jeuxvideo.com/forums/0-55-0-1-0-{}-0-politique.htm\"\n",
    "\n",
    "JV_POL, JV_18 = r\"JV\\jv_messages.csv\", r\"JV\\jv_messages_18_25.csv\"\n",
    "JV_CLEAN = r\"JV\\jv_pol_18_clean.csv\"\n",
    "\n",
    "with open(os.path.join(JVFOLDER, \"_triggers.txt\")) as f:\n",
    "    TRIGGERS = f.read().split(\"\\n\")\n",
    "POST_LIMIT = 200\n",
    "TOPIC_HEADER = ['id', 'url']\n",
    "MESSAGE_HEADER = ['id', 'parent_id', 'text', 'date']\n",
    "DATE_RE = re.compile(r\"\\d{2}\\s\\w+\\s\\d{4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_next_page(search_results):\n",
    "    \"\"\"\n",
    "    :search_results: is a BeautifulSoup object\n",
    "    \"\"\"\n",
    "    page_after = search_results.find(\"div\", attrs={\"class\":\"pagi-after\"})\n",
    "    next_ = page_after.find(\"a\")\n",
    "    if next_:\n",
    "        return next_[\"href\"]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_topics(page):\n",
    "    topics_d = []\n",
    "    topic_list = page.find(\"ul\", attrs={\"class\":\"topic-list topic-list-admin\"})\n",
    "    topics = topic_list.findAll(\"li\", attrs={\"class\":\"\"})\n",
    "    \n",
    "    for t in topics:\n",
    "        nbr_posts = t.find(\"span\", attrs={\"class\":\"topic-count\"})\n",
    "        nbr_posts = int(nbr_posts.text)\n",
    "        if nbr_posts <= POST_LIMIT:\n",
    "            id_ = t[\"data-id\"]\n",
    "            url = t.find(\"a\")[\"href\"]\n",
    "            topics_d.append({\"id\":id_,\"url\":url})\n",
    "        else:\n",
    "            continue\n",
    "    return topics_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_topic_ids(new_ids):\n",
    "    new_ids = set(new_ids)\n",
    "    with open(os.path.join(JVFOLDER, COLLECTED_TOPICS), \"r\") as f:\n",
    "        ids = f.read().split(\"\\n\")\n",
    "        ids = set(ids)\n",
    "    ids = ids.union(new_ids)\n",
    "    with open(os.path.join(JVFOLDER, COLLECTED_TOPICS), \"w\") as f:\n",
    "        f.write(\"\\n\".join(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date(message):\n",
    "    m =  re.search(DATE_RE, message.text).group()\n",
    "    if m:\n",
    "        return m\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_id(url):\n",
    "    return re.sub(r\"forums/\\d+-\\d+-(\\d+)-\\d+-\\d+-\\{\\}-\\d+-\\S+\", r\"\\1\", url)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tag(s):\n",
    "    a = re.sub(r\"<.*>\",\"\",s)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(post):\n",
    "    while re.search(g.URL, post):\n",
    "        post = re.sub(g.URL, \"\", post)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_messages(page, url):\n",
    "    messages_raw = page.findAll(\"div\", attrs={\"class\":\"bloc-message-forum\"})\n",
    "    parent_id = isolate_id(url)    \n",
    "    messages = []    \n",
    "    for m in messages_raw:\n",
    "        id_ = m[\"data-id\"]\n",
    "        \n",
    "        \n",
    "        text = m.find(\"div\",attrs={\"class\":\"txt-msg text-enrichi-forum \"})\n",
    "        bqs = text.findAll(\"blockquote\")\n",
    "        for b in bqs:\n",
    "            b.extract()\n",
    "        text = text.text\n",
    "        \n",
    "        date = find_date(m)\n",
    "\n",
    "        messages.append({\"id\":id_,\"parent_id\":parent_id,\"text\":text,\"date\":date})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_done():\n",
    "    with open(\"done_IDS.txt\", \"r\") as f:\n",
    "        id_ = f.read()\n",
    "        id_ = id_.split(sep=\"\\n\")\n",
    "        while \"\\n\" in id_:\n",
    "            id_.remove(\"\\n\")\n",
    "    return id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_done(id_):\n",
    "    file = \"done_IDS.txt\"\n",
    "    if not os.path.exists(file):\n",
    "        with open(file, \"w\") as f:\n",
    "            f.write(id_)\n",
    "    else:\n",
    "        with open(file, \"a\") as f:\n",
    "            f.write(\"\\n\"+id_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(id_):\n",
    "    done = read_done()\n",
    "    if id_ in done[::-1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_cont_table(kw, tc, rc):\n",
    "    \"\"\"\n",
    "    outputs a contingency table for the keyword \"kw\", in the target corpus \"tc\" and the reference corpus \"rc\"\n",
    "    \"\"\"\n",
    "    row1 = [tc.get(kw, 0), sum(tc.values()) - tc.get(kw, 0)]\n",
    "    row2 = [rc.get(kw, 0), sum(rc.values()) - rc.get(kw, 0)]\n",
    " \n",
    "    return [row1[0], row1[1], row2[0], row2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_freq_table(cont):\n",
    "    \"\"\"\n",
    "    outputs a frequency table for the keyword \"kw\", for the target corpus \"tc\" and the reference corpus \"rc\" (cf. kw_cont_table)\n",
    "    \"\"\" \n",
    "    r1 = cont[0] + cont[1]\n",
    "    r2 = cont[2] + cont[3]\n",
    "    c1 = cont[0] + cont[2]\n",
    "    c2 = cont[1] + cont[3]\n",
    "\n",
    "    n = r1 + r2\n",
    "    \n",
    "    a = (r1 * c1)/n\n",
    "    b = (r1 * c2)/n\n",
    "    c = (r2 * c1)/n\n",
    "    d = (r2 * c2)/n\n",
    "\n",
    "    return [a, b, c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_pmi(kw, tc, rc, freq_filter = 5):\n",
    "    \"\"\"\"\n",
    "    computes pmi value for keywords\n",
    "    \"\"\"\n",
    "    \n",
    "    cont_table = kw_cont_table(kw, tc, rc)\n",
    "    freq_table = exp_freq_table(cont_table)\n",
    "\n",
    "    if cont_table[0] >= freq_filter and kw in rc and kw in tc:\n",
    "        return math.log2(cont_table[0]/freq_table[0])\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Wiki Scraping </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1 = \"https://fr.wiktionary.org/wiki/Cat%C3%A9gorie:Insultes_en_fran%C3%A7ais\"\n",
    "URL2 = \"https://fr.wiktionary.org/w/index.php?title=Cat%C3%A9gorie:Insultes_en_fran%C3%A7ais&pagefrom=lopette#mw-pages\"\n",
    "URL3 = \"https://fr.wiktionary.org/wiki/Cat%C3%A9gorie:Insultes_en_fran%C3%A7ais_qu%C3%A9b%C3%A9cois\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w = []\n",
    "for url in [URL1,URL2,URL3]:\n",
    "    u = r.get(url, headers=HEADERS)\n",
    "    u = bs(u.text)\n",
    "    groups = u.findAll(\"div\", attrs={\"class\":\"mw-category-group\"})\n",
    "    for g in groups:\n",
    "        ws = g.findAll(\"a\")\n",
    "        all_w.extend([e.text for e in ws])\n",
    "all_w = set(all_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki_insultes.txt\",\"w\", encoding=\"utf8\") as f:\n",
    "    l = \"\\n\".join(all_w)\n",
    "    f.write(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>8chan Scraping </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vvvvvv Finding FR boards on 8chan using language detection on previously flagged boards vvvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ids = {b:set() for b in FR_BOARDS}\n",
    "for fr_board in FR_BOARDS:\n",
    "    response = r.get(ARCHIVE8.format(fr_board), headers=HEADERS)\n",
    "    if response:\n",
    "        archive_ids = re.findall(THREAD_RE,response.text)\n",
    "        archive_ids = {re.search(r\"\\d+\",e).group() for e in archive_ids}\n",
    "        fr_ids[fr_board] = fr_ids[fr_board].union(archive_ids)\n",
    "    catalog = pc.Board(fr_board)\n",
    "    catalog_ids = catalog.get_all_thread_ids()\n",
    "    catalog_ids = [str(e) for e in catalog_ids]\n",
    "    catalog_ids = set(catalog_ids)\n",
    "    fr_ids[fr_board] = fr_ids[fr_board].union(catalog_ids)\n",
    "for board,ids in fr_ids.items():\n",
    "    with open(os.path.join(FOLDER8, board)+\"_ids.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ids = dict()\n",
    "for file in os.listdir(FOLDER8):\n",
    "    with open(os.path.join(FOLDER8,file)) as f:\n",
    "        boardname = file.replace(\"_ids.txt\", \"\")\n",
    "        ids = f.read().split(\"\\n\")\n",
    "        ids = [int(e) for e in ids]\n",
    "        fr_ids[boardname] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "for board, ids in fr_ids.items():\n",
    "    b = pc.Board(board)\n",
    "    with open(os.path.join(FOLDER8, board+\".csv\"),\"w\", encoding=\"utf8\", newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(CSV_HEADER)\n",
    "        for i in ids:\n",
    "            posts = []\n",
    "            t = b.get_thread(i)\n",
    "            if t:\n",
    "                ps = t.all_posts\n",
    "                for p in ps:\n",
    "                    id_ = p.post_id\n",
    "                    if p.is_op:\n",
    "                        type_ = \"T\"\n",
    "                    else:\n",
    "                        type_ = \"R\"\n",
    "                    text= p.text_comment\n",
    "                    posts.append([id_,type_,text])\n",
    "                w.writerows(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FR 8Chan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_8chan = []\n",
    "a = all_8chan\n",
    "for fr in FR_BOARDS:\n",
    "    all_8chan.extend(grasp.csv(\"8chan_fr\\\\{}.csv\".format(fr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = a[1:]\n",
    "at = [e[2] for e in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(at):\n",
    "    e = e.strip()\n",
    "    e = remove_url(e)\n",
    "#     e = remove_html_tag(e)\n",
    "    e = e.lower()\n",
    "    e = grasp.deflood(e)\n",
    "    e = \" \".join(tokenize_fr(e))\n",
    "    at[i] = e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JeuxVideos.com Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all urls of topics to scrape from the 1st hundred pages\n",
    "nbr_pages = 100\n",
    "for i in range(1,(nbr_pages-1)*25, 25) :\n",
    "    rep = r.get(JVPOL.format(i), headers=HEADERS)\n",
    "    rep = bs(rep.text)\n",
    "    topics = get_all_topics(rep)\n",
    "    with open(os.path.join(JVFOLDER, \"topic_ids_url.csv\"),'a') as f:\n",
    "        w = csv.DictWriter(f, TOPIC_HEADER)\n",
    "        w.writerows(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading topics to scrape in memory\n",
    "with open(os.path.join(JVFOLDER, \"topic_ids_url.csv\"),'r') as f:\n",
    "        csvr = csv.DictReader(f, TOPIC_HEADER)\n",
    "        ids_url = list(csvr)\n",
    "        urls = [e['url'] for e in ids_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_urls = len(urls)\n",
    "current_url = 1\n",
    "messages = []\n",
    "for url in urls:\n",
    "    if not is_done(url):\n",
    "        if current_url%10 == 0:\n",
    "            print(\"{}/{}\".format(current_url,all_urls))\n",
    "        try:\n",
    "            url = re.sub(r\"(\\d{2}-\\d{2}-\\d+-\\d-\\d-)\\d(-\\d)\", r\"\\1{}\\2\", url)\n",
    "            rep = r.get(JVBASE+url.format(1), headers=HEADERS)\n",
    "            rep = bs(rep.text)\n",
    "\n",
    "            nbr_pages = rep.find(\"div\", attrs={\"class\":\"bloc-liste-num-page\"})\n",
    "            nbr_pages = nbr_pages.findAll(\"span\")[-1].text\n",
    "            nbr_pages = int(nbr_pages)\n",
    "\n",
    "            messages.extend(get_all_messages(rep, url))\n",
    "            if nbr_pages > 1:\n",
    "                for i in range(2,nbr_pages+1):\n",
    "                    rep = r.get(JVBASE+url.format(i), headers=HEADERS)\n",
    "                    rep = bs(rep.text)\n",
    "                    messages.extend(get_all_messages(rep, url))\n",
    "\n",
    "            file_name = \"jv_messages.csv\"\n",
    "            with open(os.path.join(JVFOLDER, file_name),\"a\",newline=\"\",encoding=\"utf8\") as f:\n",
    "                dw = csv.DictWriter(f, MESSAGE_HEADER)\n",
    "                dw.writerows(messages)\n",
    "            messages.clear()\n",
    "\n",
    "        except TimeoutError as e:\n",
    "            print(e)\n",
    "            current_url += 1\n",
    "            continue\n",
    "\n",
    "        except ConnectionError as e:\n",
    "            print(e)\n",
    "            current_url += 1\n",
    "            continue\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(current_url, url, e)\n",
    "            current_url += 1\n",
    "            continue\n",
    "    write_done(url)    \n",
    "    current_url += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JVC Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "with open(JV_POL, encoding=\"utf8\", newline=\"\") as f:\n",
    "    posts.extend(csv.reader(f))\n",
    "with open(JV_18, encoding=\"utf8\", newline=\"\") as f:\n",
    "    posts.extend(csv.reader(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [e[2] for e in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,p in enumerate(txt):\n",
    "    p = p.strip()\n",
    "    p = remove_url(p)\n",
    "    p = p.lower()\n",
    "    p = \" \".join(tokenize_fr(p))\n",
    "    txt[i] = p\n",
    "txt = [[e] for e in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JV_CLEAN, \"w\", encoding=\"utf8\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_folder = r\"D:\\4-8chan Data\\preprocessed_48\"\n",
    "files = os.listdir(final_folder)\n",
    "files = [e for e in files if e.endswith(\".csv\")]\n",
    "#4chan: text is 2nd column\n",
    "#8chan: text is 2nd column\n",
    "for f in files:\n",
    "    x = grasp.csv(os.path.join(final_folder, f))\n",
    "    if f.startswith(\"4c\"):\n",
    "        x = x[1:]\n",
    "    for i,e in enumerate(x):\n",
    "#         print(e)\n",
    "        e = e[2]\n",
    "        e = e.split()\n",
    "        x[i] = [e]\n",
    "#         print(e)\n",
    "    y = grasp.csv(os.path.join(test, f))\n",
    "    y.update(x)\n",
    "    y.save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"D:\\4-8chan Data\\preprocessed_48\\test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMS_preprocessing\n",
    "s8 = grasp.csv(SMS88, encoding=\"latin\",separator=\";\")\n",
    "sms = [e[-1] for e in s8[1:]]\n",
    "for i,e in enumerate(sms):\n",
    "    e = e.strip()\n",
    "    e = remove_url(e)\n",
    "    e = remove_html_tag(e)\n",
    "    e = e.lower()\n",
    "    e = grasp.deflood(e)\n",
    "    e = \" \".join(tokenize_fr(e))\n",
    "    sms[i] = e\n",
    "sms = [[e] for e in sms]\n",
    "sms_clean = grasp.csv(\"sms_clean.csv\", encoding=\"latin\")\n",
    "sms_clean.update(sms)\n",
    "sms_clean.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Android Sorting </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANDFILE = \"android_fr_wordlist.txt\"\n",
    "with open(ANDFILE, encoding=\"utf8\") as f:\n",
    "\traw = f.read()\n",
    "\n",
    "x = raw.split()\n",
    "x = [e for e in x if \"possibly_offensive\" in e or re.search(r\"flags=\\w,\", e)]\n",
    "d = []\n",
    "\n",
    "for line in x:\n",
    "    try:\n",
    "        word = re.search(r\"^word=(.*),f=\", line)[1]\n",
    "        print(word)\n",
    "        d.append(word)\n",
    "    except TypeError:\n",
    "        continue\n",
    "l = \"\\n\".join(d)\n",
    "\n",
    "with open(\"android_offensive_words.txt\", 'w', encoding=\"utf8\") as f:\n",
    "    f.write(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>RECAP - All insults </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for file in INSULT_FILES:\n",
    "    with open(file) as f:\n",
    "        raw = f.read()\n",
    "        words = raw.split(\"\\n\")\n",
    "        all_words.extend(words)\n",
    "all_words = set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 22>#########################</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = grasp.csv(\"sms_clean.csv\")\n",
    "jv = grasp.csv(JV_CLEAN)\n",
    "ch8 = grasp.csv(CH8_PREPROCESSED)\n",
    "\n",
    "sms= [e[0] for e in sms]\n",
    "jv= [e[0] for e in jv]\n",
    "ch8 = [e[0] for e in ch8]\n",
    "\n",
    "ch8 = [tag_fr(e) for e in ch8]\n",
    "for i,e in enumerate(ch8):\n",
    "    e = [w[0] for w in e if w[1].startswith(\"NN\") or w[1].startswith(\"JJ\") or w[1].startswith(\"VB\")]\n",
    "    ch8[i] = \" \".join(e)\n",
    "\n",
    "sms = [tag_fr(e) for e in sms]\n",
    "for i,e in enumerate(sms):\n",
    "    e = [w[0] for w in e if w[1].startswith(\"NN\") or w[1].startswith(\"JJ\") or w[1].startswith(\"VB\")]\n",
    "    sms[i] = \" \".join(e)\n",
    "    \n",
    "jv = [tag_fr(e) for e in jv]\n",
    "for i,e in enumerate(jv):\n",
    "    e = [w[0] for w in e if w[1].startswith(\"NN\") or w[1].startswith(\"JJ\") or w[1].startswith(\"VB\")]\n",
    "    jv[i] = \" \".join(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_c, jv_c, ch8_c = Counter(\" \".join(sms).split()), Counter(\" \".join(jv).split()), Counter(\" \".join(ch8).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = set(ch8_c.keys()).union(set(sms_c.keys()))\n",
    "pmi_dict = dict()\n",
    "for k in keys:\n",
    "    pmi_dict[k] = own.kw_pmi(k, ch8_c, sms_c)\n",
    "pmi_list = [e for e in pmi_dict.items()]\n",
    "pmi_list.sort(key=lambda x:x[1], reverse=True)\n",
    "print(pmi_list[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = own.csv(\"pmi_list_fr_2.csv\")\n",
    "y.update(pmi_list)\n",
    "y.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = sum([v for k,v in ch8_c.items()])/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp  = own.csv(r\"C:\\Users\\Pierre\\Downloads\\French POW - EN.csv\")\n",
    "fp = fp[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(fp):\n",
    "    fp[i][6:] = [\"FALSE\"]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = own.csv(r\"C:\\Users\\Pierre\\Downloads\\French POW - EN.csv\")\n",
    "a.clear()\n",
    "a.update(fp)\n",
    "a.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(yy):\n",
    "    e = list(e)\n",
    "    e[1] = round(int(e[1])/div)\n",
    "    yy[i] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = grasp.csv(\"insults_fr_final.csv\")\n",
    "# yy.clear()\n",
    "# yy.update(x)\n",
    "# yy.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi1 = grasp.csv(\"pmi_list_fr.csv\")\n",
    "selected = [e[0] for e in pmi1 if float(e[1]) > 0.13]\n",
    "selected = set(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "insultes = []\n",
    "for file in INSULT_FILES:\n",
    "    if not file.startswith(\"cruci\"):\n",
    "        with open(file, encoding=\"utf8\") as f:\n",
    "            insultes.extend(f.read().split(\"\\n\"))\n",
    "    else:\n",
    "        with open(file, encoding=\"latin\") as f:\n",
    "            insultes.extend(f.read().split(\"\\n\"))\n",
    "        \n",
    "insultes = set(insultes)\n",
    "selected = selected.union(insultes)\n",
    "selected = list(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ = []\n",
    "for word, nbr in x:\n",
    "    nrow = [\"\"]*18\n",
    "    nrow[8:] = [\"FALSE\"]*10\n",
    "    nrow[1] = nbr\n",
    "    nrow[3] = word\n",
    "    new_.append(nrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(k,v) for k,v in ch8_c.items() if k in selected]\n",
    "x.sort(key=lambda x:x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
